apiVersion: serving.kubeflow.org/v1alpha2
kind: InferenceService
metadata:
  name: hm-model
spec:
  default:
    predictor:
      custom:
        container:
          name: kfserving-container
          image: mokpolar/nvidia-hm:0.0.10
          resources:
            limits:
              cpu: 1
              memory: 15Gi
              nvidia.com/gpu: "1"
            requests:
              cpu: 1
              memory: 15Gi
              nvidia.com/gpu: "1"
          env:
            - name: STORAGE_URI
              value: pvc://ec2-nfs-pvc
            - name: MODEL_NAME
              value: hm-model
          command: ["python", "hm_inference.py"]
          args: ["--model_name=$(MODEL_NAME)"]
